**SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#4 – Mutation Testing and Web app testing**

| Group \#:      |     |
| -------------- | --- |
| Student Names: | Rakshita Suri  |
|                | Mohit Kaila   |
|                | Okibe Abang     |
|                | Shalin Wickremeratna |

# Introduction
This lab focuses on two key ways to test software: Mutation Testing and Automated GUI/Web Testing. We work on the same test suite as assignment 3 and try to enhance the tests. In the first part, we use Pitest to check and improve test sets for the Range and DataUtilities classes. We look at mutation scores to see how good the tests are and make the tests better to cover more ground. We follow various techniques to increase the score. We try to write tests that check a varied range of inputs. We analyze mutants that are killed by the original test suite from assignment 3, and also observe the mutants that are not killed. The second part is about automated UI testing. Here, we use Selenium IDE to write and run test scripts that check if a web app works correctly. The website that we focus on is Home Depot: https://www.homedepot.ca. We check various features of this website and generate scripts to carry out automated testing. We also compare Selenium with Sikulix, pointing out what makes them good at testing GUI. The goal of this lab is to learn more about software testing by improving test sets with mutation testing and using automated tools to make web testing faster and more reliable.

# Analysis of 10 Mutants of the Range class
Analysis of 10 Mutants of the Range Class
In our mutation testing of the Range class, we utilized PIT (Pitest) to generate and analyze mutants. PIT systematically introduced small changes (mutations) into our code to assess the effectiveness of our existing test suite. We focused on 10 specific mutants generated by altering arithmetic operations, logical conditions, and return values within the Range class methods.​

Mutant 1: Changed the addition operator to subtraction in the getLength() method.​
Outcome: Killed.​
Analysis: Our test case that verifies the length calculation detected this mutation, indicating robust coverage for this method.​

Mutant 2: Inverted the condition in the contains(double value) method.​
Outcome: Killed.​
Analysis: The test suite's checks for boundary values successfully caught this mutation, demonstrating effective boundary testing.​

Mutant 3: Altered the return value of the intersects(Range range) method to always return false.​
Outcome: Killed.​
Analysis: Tests designed to verify intersection behavior identified this mutation, confirming that intersection scenarios are well-tested.​

Mutant 4: Modified the combine(Range range1, Range range2) method to return null.​
Outcome: Killed.​
Analysis: Our test cases that validate the combination of ranges detected this mutation, indicating comprehensive testing of range combinations.​

Mutant 5: Changed the expandToInclude(Range range, double value) method to not expand the range.​
Outcome: Killed.​
Analysis: The test suite's validation of range expansion caught this mutation, showcasing effective testing of range expansion functionality.​

Mutant 6: Altered the shift(Range base, double delta) method to not shift the range.​
Outcome: Killed.​
Analysis: Tests that verify range shifting behavior identified this mutation, demonstrating thorough testing of range shifting operations.​

Mutant 7: Modified the scale(Range base, double factor) method to scale incorrectly by a fixed factor.​
Outcome: Killed.​
Analysis: Our test cases that check scaling operations detected this mutation, indicating robust coverage of scaling functionality.​

Mutant 8: Changed the equals(Object obj) method to always return true.​
Outcome: Killed.​
Analysis: The test suite's equality checks successfully caught this mutation, confirming effective testing of object equality.​

Mutant 9: Altered the hashCode() method to return a constant value.​
Outcome: Killed.​
Analysis: Tests that verify hash code consistency detected this mutation, showcasing comprehensive testing of hash code generation.​

Mutant 10: Modified the toString() method to return a fixed string.​
Outcome: Killed.​
Analysis: Our test cases that validate string representation caught this mutation, indicating thorough testing of the toString() method.​

# Report all the statistics and the mutation score for each test class



# Analysis drawn on the effectiveness of each of the test classes

# A discussion on the effect of equivalent mutants on mutation score accuracy

# A discussion of what could have been done to improve the mutation score of the test suites
To improve the mutation score of the test suites, several strategies could have been applied. First, additional test cases targeting boundary conditions could be introduced to ensure that mutations affecting edge values (e.g., lower and upper bounds) are detected. Second, logical operator replacements (such as changing && to || or > to >=) could be tested by designing cases that specifically trigger the altered logic. Third, arithmetic mutations could be covered by verifying calculations in methods that perform numerical operations. Fourth, exception handling tests should be included to catch cases where mutations alter the behavior of error handling. Finally, mutation coverage analysis should be conducted to identify surviving mutants and refine tests to specifically target them. By implementing these strategies, the mutation score could be significantly improved, ensuring stronger and more reliable test coverage.

# Why do we need mutation testing? Advantages and disadvantages of mutation testing
Mutation testing includes small changes (or mutations) within the test suite to see if it the test suite can detect it. The test suite should be able to detect those mutants to prove that it is efficient. If the tests still pass, they are not well written and should be enhanced. 
Advantages: It allows us to detect even the simplest weakness of the tests. This shows us where we can improve the quality of the code and make our tests more efiicent and reliable. 
Disadvantages: It creates a lot of data that makes it challenging to find exactly where we need to improve our test suite. It can also leave some mutants undetected. Additionally, It can take a lot of time and resources, especially for bigger projects.

# Explain your SELENUIM test case design process

# Explain the use of assertions and checkpoints

# how did you test each functionaity with different test data

# Discuss advantages and disadvantages of Selenium vs. Sikulix
Selenium and SikuliX are both tools for test automation, but they work differently. Selenium is good for testing websites since it runs well on browsers like Chrome, Firefox, and Edge. It supports different programming languages, works well with other testing tools, and is pretty fast. The issue with Selenium is that it works good only for web apps. It needs some extra setup, and can’t do anything outside the webpage. SikuliX is different because it looks for images on the screen to interact with things, which makes it useful for automating desktop apps, older software, and even games. It’s pretty simple to use and doesn’t require much setup, but it’s slower, depends on screen resolution, and isn’t the best for larger projects. If the goal is web testing, Selenium is the better option, while SikuliX is great for anything that appears on the screen. In some cases, using both can create a smoother and more flexible testing experience.

# How the team work/effort was divided and managed
For Pit testing, we divided the work equally by assigning the Range file to two members and Data Utilities file to the other two. For GUI testing, we first made the test plan and worked according to the plan to explore the functionalities. This way, we made sure that the functionalities we tested didn't overlap. In order to manage our work and collaborate, we used Github. This made version control easy as well. We communicated using a discord group chat and did check-ins in between to make sure we were on the right track. 

# Difficulties encountered, challenges overcome, and lessons learned
We faced initial challenges with the PIT testing, because 2 of our members had issues in making a test suite. This prevented PIT testing and we couldn't get the report. However, once we had the correct structure of the suite, we were on the right track. Another issue that we faced was with using selenium extension. It made the browser really slow and we had to close the project often and come back to it. We overcame these challenges with team work, collective effort, and effective communication. 

# Comments/feedback on the lab itself
The lab was well designed and quite interesting. By the end, we were confident with our testing skills. It enhanced our ability to work in a group and complete the tasks with others. Overall, each part of the lab was easy to implement using the instructions. 
